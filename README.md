# ğŸ—‘ï¸ Waste Classification with Knowledge Distillation

## ğŸ“Œ Project Overview
This project tackles the problem of **waste classification** into six categories: **Plastic, Paper, Metal, Glass, Organic, and Trash**.  
The goal was to build a **lightweight model** that can run efficiently on lowâ€‘end devices, while retaining as much accuracy as possible from a heavy teacher model.

We used **Knowledge Distillation** to transfer knowledge from a large **Inceptionâ€‘ResNetâ€‘v2** teacher (â‰ˆ85% accuracy) into a compact student CNN. The distilled student achieves **~68% accuracy** with a fraction of the parameters, making it suitable for deployment on resourceâ€‘constrained environments.

---

## ğŸ“‚ Dataset
- Source: [Trash Type Image Dataset (Kaggle)](https://www.kaggle.com/datasets/farzadnekouei/trash-type-image-dataset)  
- Total images: ~2,500  
- Split: **80% training / 20% validation**  
- Classes:  
  - Plastic  
  - Paper  
  - Metal  
  - Glass  
  - Organic  
  - Trash  

---

## ğŸ—ï¸ Methodology

### 1. Teacher Model
- **Backbone**: Inceptionâ€‘ResNetâ€‘v2 (pretrained on ImageNet).  
- Fineâ€‘tuned on the waste dataset.  
- Achieved ~83â€“85% validation accuracy.  

### 2. Student Model (Lightweight CNN)
- 4 convolutional blocks with BatchNorm + ReLU.  
- Global Average Pooling instead of flattening large feature maps.  
- Dropout (0.5) for regularization.  
- Final classifier: `Linear(256 â†’ 6)`.  
- Total parameters: **~1M** (vs. 54M in teacher).  

### 3. Knowledge Distillation
- **Soft loss**: KL divergence between teacher soft targets and student predictions.  
- **Hard loss**: Crossâ€‘entropy with ground truth labels.  
- **Final loss**:  
  

\[
  L = \alpha \cdot L_{soft} + (1-\alpha) \cdot L_{hard}
  \]

  
- Hyperparameters:  
  - Temperature (T) = 2  
  - Î± = 0.7  

---

## ğŸ“Š Results

### Validation Accuracy
- Teacher (Inceptionâ€‘ResNetâ€‘v2): **~85%**  
- Student (Distilled CNN): **~68%**

### Confusion Matrix
Shows where the student model confuses classes:

| Actual \ Predicted | Plastic | Paper | Metal | Glass | Organic | Trash |
|--------------------|---------|-------|-------|-------|---------|-------|
| Plastic            | 62      | 5     | 6     | 3     | 5       | 0     |
| Paper              | 4       | 51    | 22    | 7     | 17      | 0     |
| Metal              | 5       | 9     | 57    | 4     | 7       | 1     |
| Glass              | 4       | 1     | 15    | 91    | 8       | 0     |
| Organic            | 6       | 13    | 7     | 11    | 60      | 0     |
| Trash              | 8       | 2     | 4     | 5     | 4       | 1     |

- **Strong classes**: Glass, Plastic  
- **Confusion hotspots**: Paper â†” Metal, Glass â†” Metal  
- **Weakest class**: Trash (likely due to dataset imbalance)


### Classification Report

| Class    | Precision | Recall | F1-Score | Support |
|----------|-----------|--------|----------|---------|
| Plastic  | 0.70      | 0.77   | 0.73     | 81      |
| Paper    | 0.63      | 0.50   | 0.56     | 101     |
| Metal    | 0.50      | 0.70   | 0.58     | 82      |
| Glass    | 0.75      | 0.76   | 0.76     | 119     |
| Organic  | 0.59      | 0.62   | 0.61     | 97      |
| Trash    | 1.00      | 0.04   | 0.07     | 28      |
| **Accuracy** |        |        | **0.63** | 508     |
| **Macro Avg** | 0.69 | 0.56   | 0.55     | 508     |
| **Weighted Avg** | 0.66 | 0.63 | 0.62     | 508     |



---

## ğŸš€ Deployment Readiness
- **Model size**: ~10 MB after quantization.  
- **Inference speed**: <100ms on CPU (tested on Colab CPU).  
- Exported to **ONNX** for crossâ€‘platform deployment.  
- Can be integrated into a **mobile app** or **web demo** (Gradio/Streamlit).  

---

## ğŸ”® Future Improvements
- Collect more samples for **Trash** class to reduce imbalance.  
- Experiment with **data augmentation** (rotation, blur, brightness).  
- Try **MobileNetV2 or EfficientNetâ€‘B0** as student backbones.  
- Apply **postâ€‘training quantization** for even smaller footprint.  

---

## ğŸ“Œ Key Takeaways
- Knowledge Distillation successfully compressed a heavy teacher into a lightweight student.  
- Student retained ~80% of teacherâ€™s performance while being **5Ã— smaller and faster**.  
- This project demonstrates **endâ€‘toâ€‘end ML engineering**: dataset prep, training, distillation, evaluation, and deployment readiness.  

---

âœ¨ **Author**: Amin  
ğŸ“… **Year**: 2025  
ğŸ”— **Dataset**: [Trash Type Image Dataset (Kaggle)](https://www.kaggle.com/datasets/farzadnekouei/trash-type-image-dataset)  
